{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "import glob\n",
    "import pandas as pd\n",
    "import time\n",
    "import shutil, os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import date\n",
    "%matplotlib inline\n",
    "################ WEB SCRAPING MODULES ############\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import bs4\n",
    "from fake_useragent import UserAgent\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today() \n",
    "try:\n",
    "    os.mkdir(str(today))\n",
    "    os.mkdir(str(today)+'/Patent files')\n",
    "    os.mkdir(str(today)+'/Trials files')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get list of non-public companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(company):\n",
    "    ticker=[]\n",
    "    listed=[]\n",
    "    public=[]\n",
    "    for i in company:\n",
    "        try:\n",
    "            driver.get('https://www.google.com/search?tbm=fin&q=NASDAQ%3A%20{}'.format(\"+\".join(i.split())))\n",
    "            source = driver.page_source\n",
    "            soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "            ticker.append(soup.find('span',{'jsname':'qRSVye'}).text[0])\n",
    "            listed.append(soup.find('div',{'class':'wx62f PZPZlf'}).text.split(':')[0])\n",
    "            public.append(1)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                driver.get('https://www.google.com/search?tbm=fin&q={}'.format(\"+\".join(i.split())))\n",
    "                source = driver.page_source\n",
    "                soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "                ticker.append(soup.find('span',{'jsname':'qRSVye'}).text[0])\n",
    "                listed.append(soup.find('div',{'class':'wx62f PZPZlf'}).text.split(':')[0])\n",
    "                public.append(1)\n",
    "            except:\n",
    "                ticker.append(np.nan)\n",
    "                listed.append(np.nan)\n",
    "                public.append(0)\n",
    "        time.sleep(5)\n",
    "    return public,listed,ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[WDM] - Current google-chrome version is 88.0.4324\n",
      "[WDM] - Get LATEST driver version for 88.0.4324\n",
      " \n",
      "[WDM] - Driver [C:\\Users\\Lenovo\\.wdm\\drivers\\chromedriver\\win32\\88.0.4324.96\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install()) # open Chrome driver/window\n",
    "user=input('Do you want to enter companies one by one?(y/n): ')\n",
    "l=[]\n",
    "if(user=='y'):\n",
    "    more='y'\n",
    "    while(more=='y'):\n",
    "        a=input('enter company name: ')\n",
    "        l.append(a)\n",
    "        more=input('do you want to enter more companies(y/n): ')\n",
    "else:\n",
    "    csv=input('enter name of excel file: ') # put non-public.xlsx\n",
    "    companies=input('enter column containing list of companies: ') # put Company\n",
    "    try:\n",
    "        l=list(pd.read_csv(csv)[companies])\n",
    "    except:\n",
    "        l=list(pd.read_excel(csv)[companies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping [Google Patents](https://patents.google.com/) for Patents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sheet 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting old csv files in directories (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(directory):\n",
    "    filelist = [ f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(directory, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from \"Downloads\" directory\n",
    "delete('C:/Users/Lenovo/Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from directory created\n",
    "directory='C:/Users/Lenovo/Desktop/python scrap/'+str(today)\n",
    "delete(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from subdirectory\n",
    "delete(directory+'/Patent files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping patents for each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in l:\n",
    "    try:\n",
    "        link=\"https://patents.google.com/xhr/query?url=assignee%3D{}%26oq%3D{}&exp=&download=true\".format(\"%2B\".join(i.split()),\"%2B\".join(i.split()))\n",
    "        # download csv\n",
    "        webbrowser.open(link)\n",
    "        # wait for 20 sec\n",
    "        time.sleep(20)\n",
    "        # read csv from downloads\n",
    "        all_files = glob.glob(os.path.join(r'C:\\Users\\Lenovo\\Downloads', \"*.csv\"))\n",
    "        # rename csv\n",
    "        os.rename(all_files[0],'C:/Users/Lenovo/Downloads/'+i+'.csv')\n",
    "        # copy csv to req. folder\n",
    "        shutil.copy('C:/Users/Lenovo/Downloads/'+i+'.csv', directory+'/Patent files')\n",
    "        # delete csv from downloads\n",
    "        delete('C:/Users/Lenovo/Downloads')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying & Combining all output csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-110b0d2d919b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read all csv files in subdirectory \"Patent files\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mall_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/Patent files'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"*.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# concatenate all csv files present in \"Patent files\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_from_each_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mconcatenated_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_from_each_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# read all csv files in subdirectory \"Patent files\"\n",
    "all_files = glob.glob(os.path.join(directory+'/Patent files', \"*.csv\"))  \n",
    "# concatenate all csv files present in \"Patent files\"\n",
    "df_from_each_file = (pd.read_csv(f,header=1) for f in all_files)\n",
    "concatenated_df = pd.concat(df_from_each_file, ignore_index=True)\n",
    "concatenated_df=concatenated_df[['id','title','publication date','assignee','inventor/author']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering output based on \"assignee\"\n",
    "appended_data = []\n",
    "for company in l:\n",
    "    s = pd.Series([str(i).lower() for i in concatenated_df['assignee']])\n",
    "    k=s[s.str.contains(company.lower())].index\n",
    "    if(list(k)!=[]):\n",
    "        data = concatenated_df.loc[k]\n",
    "        data['assignee']=[company for i in range(data.shape[0])]\n",
    "        appended_data.append(data)\n",
    "appended_data = pd.concat(appended_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \"Year Filed\" from \"publication date\"\n",
    "appended_data['Year Filed']=appended_data['publication date'].apply(lambda x:str(x).split('-')[0])\n",
    "# selecting required columns\n",
    "appended_data=appended_data[['assignee','id', 'title', 'publication date','Year Filed','inventor/author']]\n",
    "# renaming columns\n",
    "appended_data.columns=['Company Name/Assignee','Patent No.','Title','Publication Date','Year Filed','Inventors/Authors']\n",
    "appended_data['Year Filed']=appended_data['Year Filed'].replace('nan',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Scraped Patents using [USPTO](http://patft.uspto.gov/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install()) # open Chrome driver/window\n",
    "links=[]\n",
    "for i in l:\n",
    "    term=\"+\".join(i.split())\n",
    "    driver.get('http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=0&f=S&l=50&TERM1={}&FIELD1=ASNM&co1=AND&TERM2=&FIELD2=&d=PTXT'.format(term))\n",
    "    source = driver.page_source\n",
    "    soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "    #################### GET LINKS OF PAGES FOR EACH COMPANY #####################\n",
    "    try:\n",
    "        no_of_pages=int(soup.find_all('strong')[len(soup.find_all('strong'))-1].text)\n",
    "        for page in range(no_of_pages):\n",
    "            link='http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r={}&f=G&l=50&co1=AND&d=PTXT&s1=%22{}%22.ASNM.&OS=AN/%22{}al%22&RS=AN/%22{}%22'.format(page+1,term,term,term)\n",
    "            links.append(link)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title=[]\n",
    "for i,j in enumerate(links):\n",
    "    print(i)\n",
    "    driver.get(j)\n",
    "    source = driver.page_source\n",
    "    soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "    try:\n",
    "        text=soup.find('font',{'size':'+1'}).text.strip()\n",
    "        print(text)\n",
    "        title.append(text)\n",
    "    except:\n",
    "        pass\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data['Title']=appended_data['Title'].apply(lambda x:x.strip())\n",
    "appended_data=appended_data.loc[appended_data['Title'].isin(list(set(title)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data=appended_data.drop_duplicates(subset=['Company Name/Assignee', 'Title'])\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping [Clinical Trials](https://clinicaltrials.gov/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in l:\n",
    "    try:\n",
    "        link=\"https://clinicaltrials.gov/ct2/results/download_fields?cond=&term={}&type=&down_fmt=csv&down_flds=all&rslt=&age_v=&gndr=&intr=&titles=&outc=&spons=&lead=&id=&cntry=&state=&city=&dist=&locn=&fund=2&rsub=&strd_s=&strd_e=&prcd_s=&prcd_e=&sfpd_s=&sfpd_e=&rfpd_s=&rfpd_e=&lupd_s=&lupd_e=&sort=\".format(\"+\".join(i.split()))\n",
    "        df = pd.read_csv(link)\n",
    "        df=df[['First Posted','NCT Number','Title','Start Date','Completion Date','Sponsor/Collaborators']]\n",
    "        df['Company Name']=[i for j in range(df.shape[0])]\n",
    "        df.to_csv(directory+'/Trials files/'+i+'.csv',index=False)\n",
    "    except:\n",
    "        c=c+1\n",
    "        #print(c,i+'.csv','not scraped')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join(directory+'/Trials files', \"*.csv\"))  \n",
    "if(all_files!=[]):\n",
    "    a=pd.read_csv(all_files[0])\n",
    "    for i in all_files[1:]:\n",
    "        a=pd.concat([a,pd.read_csv(i)])\n",
    "        \n",
    "    a['Principal Investigator']=['-' for j in range(a.shape[0])]\n",
    "    a=a[['Company Name','NCT Number','Title', 'Start Date', 'Completion Date','Sponsor/Collaborators','Principal Investigator']] \n",
    "    a.columns=['Company Name','Clinical Trial Identifier','Title of study',\n",
    "               'Start Date','End Date','Sponsor Company','Principal Investigator']\n",
    "    a['C'] = a.apply(lambda x: str(x['Company Name']) in str(x['Sponsor Company']), axis=1)\n",
    "    a=a.loc[a['C']==True]\n",
    "    a=a.drop('C',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(all_files!=[]):\n",
    "    ua = UserAgent()\n",
    "    for i in range(a.shape[0]):\n",
    "        url='https://clinicaltrials.gov/ct2/show/{}'.format(a.iloc[i,1])\n",
    "        response = requests.get(url, {\"User-Agent\": ua.random})\n",
    "        if response.status_code == 200:\n",
    "            soup = bs4.BeautifulSoup(response.text, \"html.parser\")\n",
    "            for j in soup.find_all('table',{'class':'ct-layout_table tr-indent2'}):\n",
    "                m=j.text\n",
    "                if('Study Director:' in m):\n",
    "                    a.iloc[i,-1]=m[m.index('Study Director:')+16:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    d_range=input('Do you want for a date range(y/n): ')\n",
    "    if(d_range=='y'):\n",
    "        date1 = input('From(YYYY-MM-DD): ')\n",
    "        date2 = input('To(YYYY-MM-DD): ')\n",
    "        dates=[]\n",
    "        dates2=[]\n",
    "        start = datetime.datetime.strptime(date1, '%Y-%m-%d')\n",
    "        end = datetime.datetime.strptime(date2, '%Y-%m-%d')\n",
    "        step = datetime.timedelta(days=1)\n",
    "    ################################################################\n",
    "        while start <= end:\n",
    "            dates.append (str(start.date()))\n",
    "            start += step\n",
    "        for i in dates:\n",
    "            a1=i.split('-')\n",
    "            x = datetime.datetime(int(a1[0]),int(a1[1]),int(a1[2]))\n",
    "            dates2.append(x.strftime(\"%B %d, %Y\"))\n",
    "    ##################################################################\n",
    "        appended_data=appended_data.loc[appended_data['Publication Date'].isin(dates)]\n",
    "        a=a.loc[a['Start Date'].isin(dates2)]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Adding not-scraped companies to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents\n",
    "not_scraped=list(set(l)-set(appended_data['Company Name/Assignee'].unique()))\n",
    "for i in not_scraped:\n",
    "    s2 = pd.DataFrame([i,np.nan,np.nan,np.nan,np.nan,np.nan]).T\n",
    "    s2.columns=appended_data.columns\n",
    "    appended_data=appended_data.append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials\n",
    "if(all_files!=[]):\n",
    "    not_scraped=list(set(l)-set(a['Company Name'].unique()))\n",
    "    for i in not_scraped:\n",
    "        s2 = pd.DataFrame([i,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]).T\n",
    "        s2.columns=a.columns\n",
    "        a=a.append(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patents\n",
    "appended_data2=appended_data.groupby(['Company Name/Assignee']).size().reset_index(name='Value')\n",
    "appended_data2['Patent (Y/N)']=['Y' for i in range(appended_data2.shape[0])]\n",
    "for i in range(appended_data2.shape[0]):\n",
    "    if(appended_data2['Company Name/Assignee'][i] in not_scraped):\n",
    "        appended_data2['Value'][i]=0\n",
    "        appended_data2['Patent (Y/N)'][i]='N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials\n",
    "appended_data3=a.groupby(['Company Name']).size().reset_index(name='Value')\n",
    "appended_data3['Clinical Trials (Y/N)']=['Y' for i in range(appended_data3.shape[0])]\n",
    "for i in range(appended_data3.shape[0]):\n",
    "    if(appended_data3['Company Name'][i] in not_scraped):\n",
    "        appended_data3['Value'][i]=0\n",
    "        appended_data3['Clinical Trials (Y/N)'][i]='N'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Sheet 1 and Sheet 2 as excel\n",
    "name='/BiosectRx Patent Harvest-'+str(today)+'.xlsx'\n",
    "writer = pd.ExcelWriter(directory+name, engine='xlsxwriter')\n",
    "#appended_data.reset_index(inplace=True)\n",
    "#appended_data2.reset_index(inplace=True)\n",
    "appended_data2.to_excel(writer, sheet_name='Patent Check',index=False)\n",
    "appended_data.to_excel(writer, sheet_name='Harvest',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name='/BiosectRx Trials Harvest-'+str(today)+'.xlsx'\n",
    "writer = pd.ExcelWriter(directory+name, engine='xlsxwriter')\n",
    "a.reset_index(inplace=True)\n",
    "appended_data3.reset_index(inplace=True)\n",
    "appended_data3.to_excel(writer, sheet_name='Trials Check',index=False)\n",
    "a.to_excel(writer, sheet_name='Harvest',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate in Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge patents and trials\n",
    "df3=pd.merge(appended_data, a, left_on='Company Name/Assignee', right_on='Company Name')\n",
    "df3=df3.drop(['Company Name'],axis=1)\n",
    "df3=df3.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open original template to get format\n",
    "df=pd.read_excel('Template.xlsx')\n",
    "df.columns=df.iloc[0,:]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df3.copy()\n",
    "df5=df5[['Company Name/Assignee', 'Patent No.', 'Title', 'Year Filed',\n",
    "       'Inventors/Authors', 'Clinical Trial Identifier',\n",
    "       'Title of study', 'Start Date', 'End Date', 'Sponsor Company',\n",
    "       'Principal Investigator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df5.columns=['Company Name','Application No','IP Title','IP Year',\n",
    "             'Inventors','Clinical Trial ID','Clinical Title',\n",
    "            'Start Date', 'End Date', 'Sponsor', 'Investigator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df5.iloc[:,:5]\n",
    "dup_index=set(df6.index)-set(df6.drop_duplicates(keep='first').index)\n",
    "df6.iloc[list(dup_index),1:]=np.NaN\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=df5.iloc[:,5:]\n",
    "df7['a']=df5['Company Name']\n",
    "dup_index=set(df7.index)-set(df7.drop_duplicates(keep='first').index)\n",
    "df7.iloc[list(dup_index)]=np.NaN\n",
    "df7=df7.drop('a',axis=1)\n",
    "df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.concat([df6,df7],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add null columns as per template\n",
    "for i in set(df.columns)-set(df5.columns):\n",
    "    x[i]=np.nan\n",
    "    \n",
    "# rearrange columns as per template\n",
    "x=x[df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x.groupby('Company Name').count()[['Application No','Clinical Trial ID']]\n",
    "l1,l2=[],[]\n",
    "for i in y['Application No']:\n",
    "    if(i==0):\n",
    "        l1.append('N')\n",
    "    else:\n",
    "        l1.append('Y')\n",
    "for i in y['Clinical Trial ID']:\n",
    "    if(i==0):\n",
    "        l2.append('N')\n",
    "    else:\n",
    "        l2.append('Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['Patent (Y/N)']=l1\n",
    "y['Clinical Trials (Y/N)']=l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y[['Patent (Y/N)','Application No','Clinical Trials (Y/N)','Clinical Trial ID']]\n",
    "y.columns=['Patent (Y/N)','Value','Clinical Trials (Y/N)','Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.sort_values(by=['Patent (Y/N)', 'Clinical Trials (Y/N)'])\n",
    "ordered_classes = list(y.index)\n",
    "df_list = []\n",
    "for i in ordered_classes:\n",
    "    df_list.append(x[x['Company Name']==i])\n",
    "ordered_df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to excel\n",
    "name='/BiosectRx TEMPLATE-'+str(today)+'.xlsx'\n",
    "writer = pd.ExcelWriter(directory+name, engine='xlsxwriter')\n",
    "y.to_excel(writer, sheet_name='Check',index=True)\n",
    "x.to_excel(writer, sheet_name='Harvest',index=False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}