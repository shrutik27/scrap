{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ WEB SCRAPING MODULES ############\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import bs4\n",
    "from fake_useragent import UserAgent\n",
    "import requests\n",
    "################ TIME MODLULES ###################\n",
    "import time\n",
    "from datetime import date \n",
    "import datetime\n",
    "############## DATA MANIPULATION MODULES #########\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import statistics \n",
    "from statistics import mode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_and_text():\n",
    "    link=[i.get('href') for i in soup.find_all('a',{'style':'text-decoration:none;display:block'})]\n",
    "    text=[i.text for i in soup.find_all('div',{'class':'JheGif nDgy9d'})]\n",
    "    date=[i.text for i in soup.find_all('span',{'class':'WG9SHc'})]\n",
    "    website=[i.text for i in soup.find_all('div',{'class':'XTjFC WF4CUc'})]\n",
    "    ##################### FILTER VALUES ########################################\n",
    "    INDEX_1=[i for i in range(len(text)) if '$' in text[i]]\n",
    "    link=[link[i] for i in INDEX_1]\n",
    "    text=[text[i] for i in INDEX_1]\n",
    "    date=[date[i] for i in INDEX_1]\n",
    "    website=[website[i] for i in INDEX_1]\n",
    "    ############################################################################\n",
    "    return link,text,date,website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_amount(z):\n",
    "    key=['million','billion','millions','billions','trillion','trillions']\n",
    "    amount=[]\n",
    "    for x in z:\n",
    "        try:\n",
    "            if(x[x.index('$'):].split()[1].lower() in key):\n",
    "                amount.append(x[x.index('$'):].split()[0]+' '+x[x.index('$'):].split()[1])\n",
    "            else:\n",
    "                amount.append(x[x.index('$'):].split()[0])\n",
    "        except:\n",
    "            amount.append(x[x.index('$'):].split()[0])\n",
    "    return amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_non_public(company):\n",
    "    ticker=[]\n",
    "    listed=[]\n",
    "    public=[]\n",
    "    for i in company:\n",
    "        try:\n",
    "            driver.get('https://www.google.com/search?tbm=fin&q=NASDAQ%3A%20{}'.format(\"+\".join(i.split())))\n",
    "            source = driver.page_source\n",
    "            soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "            ticker.append(soup.find('span',{'jsname':'qRSVye'}).text[0])\n",
    "            listed.append(soup.find('div',{'class':'wx62f PZPZlf'}).text.split(':')[0])\n",
    "            public.append(1)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "            try:\n",
    "                driver.get('https://www.google.com/search?tbm=fin&q={}'.format(\"+\".join(i.split())))\n",
    "                source = driver.page_source\n",
    "                soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "                ticker.append(soup.find('span',{'jsname':'qRSVye'}).text[0])\n",
    "                listed.append(soup.find('div',{'class':'wx62f PZPZlf'}).text.split(':')[0])\n",
    "                public.append(1)\n",
    "            except:\n",
    "                ticker.append(np.nan)\n",
    "                listed.append(np.nan)\n",
    "                public.append(0)\n",
    "        time.sleep(5)\n",
    "    return public,listed,ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=input('Do you want to enter companies one by one?(y/n): ')\n",
    "l=[]\n",
    "if(user=='y'):\n",
    "    more='y'\n",
    "    while(more=='y'):\n",
    "        a=input('enter company name: ')\n",
    "        l.append(a)\n",
    "        more=input('do you want to enter more companies(y/n): ')\n",
    "else:\n",
    "    csv=input('enter name of excel file: ')\n",
    "    companies=input('enter column containing list of companies: ')\n",
    "    try:\n",
    "        l=list(pd.read_csv(csv)[companies])\n",
    "    except:\n",
    "        l=list(pd.read_excel(csv)[companies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['InterAx Biotech AG',\n",
       " 'Nanolive SA',\n",
       " 'Biognosys AG',\n",
       " 'Clemedi',\n",
       " 'deepCDR Biologics',\n",
       " 'ABCDx SA',\n",
       " 'Scailyte AG',\n",
       " 'Invasight',\n",
       " 'SensArs Neuroprosthetics Sàrl',\n",
       " 'Vigilitech AG']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Taking 10/424 companies for example\n",
    "l=l[:10]\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape [Google News](https://news.google.com/topstories?hl=en-IN&gl=IN&ceid=IN:en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[WDM] - Current google-chrome version is 88.0.4324\n",
      "[WDM] - Get LATEST driver version for 88.0.4324\n",
      " \n",
      "[WDM] - Get LATEST driver version for 88.0.4324\n",
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/88.0.4324.96/chromedriver_win32.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\Lenovo\\.wdm\\drivers\\chromedriver\\win32\\88.0.4324.96]\n",
      "InterAx Biotech AG\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Nanolive SA\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  1 1 1 1\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Biognosys AG\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Clemedi\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "deepCDR Biologics\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "ABCDx SA\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Scailyte AG\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Invasight\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "SensArs Neuroprosthetics Sàrl\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n",
      "Vigilitech AG\n",
      "#############################################\n",
      "Page No. 1\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 2\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 3\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 4\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "Page No. 5\n",
      "No. of links and text are:  0 0 0 0\n",
      "**************************************************\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "x,k,y,z,m=[],[],[],[],[]\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install()) # open Chrome driver/window\n",
    "for i in l:\n",
    "    print(i) # print company being scraped\n",
    "    print('#'*45)\n",
    "    # Scrape first 5 pages of \"<COMPANY NAME> raises financing\"\n",
    "    for j in range(0,50,10): \n",
    "        driver.get('https://www.google.com/search?&tbm=nws&q={}+raises+financing&start={}'.format(\"+\".join(i.split()),j))\n",
    "        source = driver.page_source\n",
    "        soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "        link,text,date,website=get_links_and_text()\n",
    "        print('Page No.',int(str(j)[0])+1)\n",
    "        print('No. of links and text are: ',len(link),len(text),len(date),len(website))\n",
    "        x.append(date)\n",
    "        y.append(link)\n",
    "        z.append(text)\n",
    "        k.append(website)\n",
    "        m.append([i for j in range(len(website))])\n",
    "        time.sleep(5)\n",
    "        print('*'*50)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "x=[j for i in x for j in i]\n",
    "k=[j for i in k for j in i]\n",
    "y=[j for i in y for j in i]\n",
    "z=[j for i in z for j in i]\n",
    "m=[j for i in m for j in i]\n",
    "# Get amount from title\n",
    "amount=scrape_amount(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from scraped features\n",
    "df=pd.DataFrame(zip(m,x,z,y,amount),\n",
    "                columns=['Company Name','Date','Article Title','URL','Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only those rows where Company Name is in Article Title\n",
    "df['temp']=df['Company Name'].apply(lambda x: x.split()[0])\n",
    "df['C'] = df.apply(lambda x: str(x['temp']) in str(x['Article Title']), axis=1)\n",
    "df=df.loc[df['C']==True].drop(['temp','C'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse each article in DataFrame and scrape investors and series type\n",
    "investors=[]\n",
    "series=[]\n",
    "for j,url in enumerate(df['URL']):\n",
    "    print('Article',j)\n",
    "    driver.get(url)\n",
    "    source = driver.page_source\n",
    "    soup=bs4.BeautifulSoup(source, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    \n",
    "    a = soup.get_text()\n",
    "    a=\" \".join(a.split())\n",
    "    a=re.sub('[^A-Za-z0-9.]+', ' ', a)\n",
    "    investors.append([i for i in a.split('.') if bool(re.search('investors ',i.lower()))==True])\n",
    "    series.append([i.lower()[i.lower().find('series'):i.lower().find('series')+8] for i in a.split('.') if i.lower().find('series')!=-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize investors and series type lists\n",
    "series2=[]\n",
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count) \n",
    "\n",
    "for i in range(len(investors)):\n",
    "    if(investors[i]==[]):\n",
    "        investors[i]=np.nan\n",
    "\n",
    "for i in series:\n",
    "    try:\n",
    "        series2.append(most_frequent(i))\n",
    "    except:\n",
    "        series2.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lists to DataFrame\n",
    "df['Series Type']=series2\n",
    "df['Investors']=investors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all values NaN in column \"Series Type\", except those which are in list allowed_vals\n",
    "allowed_vals=['series b', 'series d', 'series a', 'series c','series e']\n",
    "df['Series Type'][~df['Series Type'].isin(allowed_vals)] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arranging Columns of DataFrame\n",
    "df=df[['Company Name', 'Date', 'Article Title', 'URL', 'Series Type', 'Amount',\n",
    "       'Investors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    d_range=input('Do you want for a date range(y/n): ')\n",
    "    if(d_range=='y'):\n",
    "        date1 = input('From(YYYY-MM-DD): ')\n",
    "        date2 = input('To(YYYY-MM-DD): ')\n",
    "        dates=[]\n",
    "        dates2=[]\n",
    "        start = datetime.datetime.strptime(date1, '%Y-%m-%d')\n",
    "        end = datetime.datetime.strptime(date2, '%Y-%m-%d')\n",
    "        step = datetime.timedelta(days=1)\n",
    "    ################################################################\n",
    "        while start <= end:\n",
    "            dates.append (str(start.date()))\n",
    "            start += step\n",
    "        for i in dates:\n",
    "            a1=i.split('-')\n",
    "            x = datetime.datetime(int(a1[0]),int(a1[1]),int(a1[2]))\n",
    "            dates2.append(x.strftime(\"%b %d, %Y\"))\n",
    "    ##################################################################\n",
    "        df=df.loc[df['Date'].isin(dates2)]\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding not-scraped companies to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=df['Company Name'].unique()\n",
    "not_scraped=list(set(l)-set(k))\n",
    "for i in not_scraped:\n",
    "    s2 = pd.DataFrame([i,np.nan,np.nan,np.nan,np.nan,np.nan,np.nan]).T\n",
    "    s2.columns=df.columns\n",
    "    df=df.append(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10, 7)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.groupby(['Company Name']).size().reset_index(name='Value')\n",
    "df2['Financing Rounds (Y/N)']=['Y' for i in range(df2.shape[0])]\n",
    "for i in range(df2.shape[0]):\n",
    "    if(df2['Company Name'][i] in not_scraped):\n",
    "        df2['Value'][i]=0\n",
    "        df2['Financing Rounds (Y/N)'][i]='N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    Company Name  Value Financing Rounds (Y/N)\n",
       "0                       ABCDx SA      0                      N\n",
       "1                   Biognosys AG      0                      N\n",
       "2                        Clemedi      0                      N\n",
       "3             InterAx Biotech AG      0                      N\n",
       "4                      Invasight      0                      N\n",
       "5                    Nanolive SA      0                      N\n",
       "6                    Scailyte AG      0                      N\n",
       "7  SensArs Neuroprosthetics Sàrl      0                      N\n",
       "8                  Vigilitech AG      0                      N\n",
       "9              deepCDR Biologics      0                      N"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Company Name</th>\n      <th>Value</th>\n      <th>Financing Rounds (Y/N)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABCDx SA</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Biognosys AG</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Clemedi</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>InterAx Biotech AG</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Invasight</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Nanolive SA</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Scailyte AG</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>SensArs Neuroprosthetics Sàrl</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Vigilitech AG</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>deepCDR Biologics</td>\n      <td>0</td>\n      <td>N</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "name='BiosectRx Financing Harvest-'+str(date.today())+'.xlsx'\n",
    "writer = pd.ExcelWriter(name, engine='xlsxwriter')\n",
    "df2.to_excel(writer, sheet_name='Financing Check',index=False)\n",
    "df.to_excel(writer, sheet_name='Harvest',index=False)\n",
    "writer.save()\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}